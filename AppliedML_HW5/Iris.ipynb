{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data exploration:\n",
    "from sklearn import datasets\n",
    "data = datasets.load_iris(return_X_y=False,as_frame=True)\n",
    "print(data.data.head())\n",
    "features_name=data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical analysis\n",
    "features=data.data\n",
    "classes=data.target_names\n",
    "target=data.target\n",
    "Iris=features.copy()\n",
    "Iris['target']=target\n",
    "Iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore 3 classes \n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iris.corr()['target'].sort_values(ascending=False)\n",
    "Iris[\"petal_sepal__length_ratio\"] = Iris[\"sepal length (cm)\"] / Iris[\"petal length (cm)\"]\n",
    "Iris[\"petal_sepal__width_ratio\"] = Iris[\"sepal width (cm)\"] / Iris[\"petal width (cm)\"]\n",
    "Iris.corr()['target'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot each feature among the 3 categorical classes:\n",
    "from matplotlib import pyplot as plt\n",
    "#print(Iris.head())\n",
    "for col in features_name:\n",
    "    Iris.boxplot(column=col,by='target', figsize=(6,6))\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data imbalanced:\n",
    "Iris.groupby('target').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data Imbalanced\n",
    "Iris['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of samples,and chek null values\n",
    "Iris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=features\n",
    "y=target\n",
    "Xtrain,Xtest,ytrain,ytest=train_test_split( X,y, test_size=0.2,random_state=42,shuffle=True)\n",
    "Xtrain.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline,FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"]  # feature names out\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        #StandardScaler()\n",
    "        )\n",
    "num_pipeline=make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        #StandardScaler()\n",
    "        )\n",
    "preprocessing = ColumnTransformer([\n",
    "        (\"widthratio\", ratio_pipeline(), [\"petal width (cm)\", \"sepal width (cm)\"]),\n",
    "        (\"lengthratio\", ratio_pipeline(), [\"petal length (cm)\", \"sepal length (cm)\"]),\n",
    "        (\"features\",num_pipeline,[\"petal width (cm)\", \"sepal width (cm)\",\"petal length (cm)\", \n",
    "        \"sepal length (cm)\"])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import SGDClassifier,LogisticRegressionCV,LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "clf={\n",
    "    'SGDClassifier':SGDClassifier(random_state=42,alpha=.1,max_iter=30000,tol=.0001,loss='squared_hinge'),\n",
    "    'KNeighborsClassifier':KNeighborsClassifier(n_neighbors=3),\n",
    "    'DecisionTreeClassifier':DecisionTreeClassifier(random_state=42,max_features=3),\n",
    "    'RandomForestClassifier':RandomForestClassifier(random_state=42,max_features=3),\n",
    "    'LinearSVC':LinearSVC(random_state=42,max_iter=30000,class_weight='balanced',\n",
    "                            multi_class=\"crammer_singer\",C=1,tol=.001),\n",
    "    'SVC':  SVC(C=1,max_iter=1000,tol=.001),\n",
    "    'logistic_reg':  LogisticRegression(l1_ratio=1,C=1,penalty=\"elasticnet\",max_iter=1000,solver=\"saga\",\n",
    "                                        random_state=42,tol=.0004),\n",
    "    #'VotingClassifier':VotingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the score for multible models and find the best estimator:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "results=[]\n",
    "for key in clf.keys():\n",
    "    full_pipeline = make_pipeline(preprocessing, clf[key])\n",
    "    score=cross_val_score(full_pipeline, Xtrain, ytrain, scoring=\"accuracy\", cv=3)\n",
    "    results.append((key,score.mean()*100))\n",
    "print('models scores:',results)\n",
    "best_model_idx=np.array(results)[:,1].argmax()\n",
    "print('best model:',results[best_model_idx][0],results[best_model_idx][1].round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the linear svc on test dataset\n",
    "score=cross_val_score(clf['LinearSVC'], Xtrain, ytrain, scoring=\"accuracy\", cv=3)\n",
    "score.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=cross_val_score(clf['logistic_reg'], Xtrain, ytrain, scoring=\"accuracy\", cv=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try the voting classifier model for the highest 3 models :\n",
    "eclf = VotingClassifier([(\"lsvc\", clf['LinearSVC']),\n",
    "     (\"log_reg\", clf['logistic_reg'])],voting='hard')#,weights=[1,2,1])\n",
    "eclf.fit(Xtrain, ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the score(accuracy)\n",
    "score=cross_val_score(eclf,Xtrain,ytrain,scoring='accuracy',cv=3)\n",
    "accuracy=score.mean()\n",
    "print('accuracy percentage:',accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find confusuin matrix for the three classes\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "ytrain_pred = cross_val_predict(eclf, Xtrain, ytrain, cv=3)\n",
    "cm = confusion_matrix(ytrain, ytrain_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusuin matrix for the three classes\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(ytrain, ytrain_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(ytrain, ytrain_pred,average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall\n",
    "recall_score(ytrain, ytrain_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find f1score:\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(ytrain, ytrain_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the Classification report and scoring details for each class:\n",
    "from sklearn.metrics import classification_report,confusion_matrix,multilabel_confusion_matrix\n",
    "import pandas as pd\n",
    "report=classification_report(ytrain, ytrain_pred,target_names=data.target_names,output_dict=True)\n",
    "df = pd.DataFrame.from_dict(report)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on dataset:\n",
    "from sklearn.metrics import accuracy_score\n",
    "#score=cross_val_score(eclf,Xtest,ytest,scoring='accuracy',cv=3)\n",
    "ytest_pred = eclf.predict(Xtest)\n",
    "\n",
    "score=accuracy_score(ytest,ytest_pred)\n",
    "accuracy=score.mean()*100\n",
    "print('accuracy percentage:',accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusuin matrix for the three classes\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "#ytest_pred = cross_val_predict(eclf, Xtest, ytest, cv=3)\n",
    "cm = confusion_matrix(ytest, ytest_pred)\n",
    "cm\n",
    "ConfusionMatrixDisplay.from_predictions(ytest, ytest_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=classification_report(ytest, ytest_pred,target_names=data.target_names,output_dict=True)\n",
    "df = pd.DataFrame.from_dict(report)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the best model in the whole data  and Save the final Model:\n",
    "import joblib\n",
    "\n",
    "final_model=eclf.fit(features,target)\n",
    "\n",
    "joblib.dump(final_model,'clf_final_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model for prediction\n",
    "final_model=joblib.load('clf_final_model.pkl')\n",
    "new_data=features.iloc[:5]\n",
    "predictions=final_model.predict(new_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perfect!! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    eclf, X, y, train_sizes=np.linspace(0.01, 1.0, 140), cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\")\n",
    "train_errors = -train_scores.mean(axis=1)\n",
    "valid_errors = -valid_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n",
    "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "\n",
    "# extra code – beautifies and saves Figure 4–15\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.axis([80, 140, 0, .2])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
