{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data = datasets.load_diabetes(return_X_y=False,as_frame=True)\n",
    "#Exploring the Data:\n",
    "print(data.data.head())\n",
    "features_name=data.feature_names\n",
    "print(features_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#remove duplications in features\n",
    "duplication=data.data.duplicated().value_counts()\n",
    "duplication_sum=data.data.duplicated().sum()\n",
    "new_data=data.data.iloc[data.data.duplicated(keep='last').index]\n",
    "features=data.data\n",
    "target=data.target\n",
    "duplication\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of samples and check null values:\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the null on the label column:\n",
    "target.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do satatitical analysis to find the mean , min , and max values :\n",
    "num_features=['age', 'bmi', 'bp', 's1', 's2', 's3', 's5', 's6']\n",
    "cat_features=['sex','s4']\n",
    "Diabetes=features.copy()\n",
    "Diabetes['target']=target\n",
    "Diabetes[num_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Outliers by using boxplot :\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "type(features)\n",
    "for col in num_features:\n",
    "    Diabetes.boxplot(column=col, figsize=(6,6))\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the distripution of each numarical feature :\n",
    "#the data is normalized between (-0.1,0.1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Diabetes[num_features].hist(bins=50,figsize=(12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates Figure 2–17\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
    "Diabetes[\"bmi\"].hist(ax=axs[0], bins=50)\n",
    "Diabetes[\"bmi\"].apply(np.log).hist(ax=axs[1], bins=50)\n",
    "axs[0].set_xlabel(\"pmi\")\n",
    "axs[1].set_xlabel(\"Log of pmi\")\n",
    "axs[0].set_ylabel(\"Number of diabetes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "percentiles = [np.percentile(Diabetes[\"bmi\"], p)\n",
    "               for p in range(0,10)]\n",
    "flattened_median_income = pd.cut(Diabetes[\"bmi\"],\n",
    "                                 bins=[-np.inf] + percentiles + [np.inf],\n",
    "                                 labels=range(0, 10 + 1))\n",
    "flattened_median_income.hist(bins=1)\n",
    "plt.xlabel(\"Median income percentile\")\n",
    "plt.ylabel(\"Number of districts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for feature in features_name:\n",
    "    Diabetes.plot(kind='scatter',x=feature,y='target',grid=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "attributes=['target','bmi','bp','s1','s2','s3','s5','s6','age']\n",
    "scatter_matrix(Diabetes[attributes],figsize=(12,8))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#there is a high correlation between the two features s1, s2 (not good) to solve that we can use PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation=Diabetes[attributes].corr()\n",
    "correlation['target'].sort_values(ascending=False)\n",
    "#the target is highly correlated with the most of the num features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation['s1'].sort_values(ascending=False)#there is ahigh correlation between the two \n",
    "#features(s1,s2 and also s5) we can solve this by applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=features\n",
    "y=target\n",
    "Xtrain,Xtest,ytrain,ytest=train_test_split( X,y, test_size=0.2,stratify=features[\"sex\"],random_state=42)\n",
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "#Apply PCA to solve the high correlation between features\n",
    "pca = PCA(n_components=6)\n",
    "Xtrain_num = pca.fit_transform(features[num_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn .utils.validation import check_array,check_is_fitted\n",
    "from sklearn.cluster import KMeans\n",
    "#apply clustering similarity for the S4 feature\n",
    "class ClusterSimilarity(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,n_clusters=6,gamma=0.1,random_state=None):\n",
    "        self.n_clusters=n_clusters\n",
    "        self.gamma=gamma\n",
    "        self.random_state=random_state\n",
    "\n",
    "    def fit(self,X,sample_weight=None,y=None):\n",
    "        X=check_array(X)\n",
    "        self.kmeans_=KMeans(n_clusters=self.n_clusters,random_state=self.random_state)\n",
    "        self.kmeans_.fit(X,sample_weight=sample_weight)\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        check_is_fitted(self)\n",
    "        X=check_array(X)\n",
    "        assert self.n_features_in_==X.shape[1]\n",
    "        \n",
    "        return rbf_kernel(X,self.kmeans_.cluster_centers_,gamma=self.gamma) \n",
    "\n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cluster_simil = ClusterSimilarity(n_clusters=6, gamma=1., random_state=42)\n",
    "similarities = cluster_simil.fit_transform(np.array(np.array(Diabetes['s4']).reshape(-1, 1)),\n",
    "                                           sample_weight=target)\n",
    "cluster_simil.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a boxplot for the label by each categorical feature\n",
    "categorical_features=['sex','s4']\n",
    "for col in categorical_features:\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    ax = fig.gca()\n",
    "    Diabetes.boxplot(column = 'target', by = col, ax = ax)\n",
    "    ax.set_title('Label by ' + col)\n",
    "    ax.set_ylabel(\"Diabets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import IsolationForest\n",
    "#\n",
    "#outlier_pred=IsolationForest(random_state=42)\n",
    "#out=outlier_pred.fit_predict(features)\n",
    "#Diabetes = Diabetes.iloc[out == 1]\n",
    "#target = target.iloc[out == 1]\n",
    "#Diabetes.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer,make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "results=[] \n",
    "reg={'LinearRegression':LinearRegression(),\n",
    "    'KNeighborsRegressor':KNeighborsRegressor(),\n",
    "    'DecisionTreeRegressor':DecisionTreeRegressor(random_state=42),\n",
    "    'RandomForestRegressor':RandomForestRegressor(random_state=42,max_features=10,criterion='poisson'),\n",
    "    'LinearSVR':LinearSVR(random_state=42),\n",
    "    'SVR':  SVR()}\n",
    "\n",
    "\n",
    "num_pipline=make_pipeline(PCA(n_components=6))\n",
    "s4_cluster_simil=make_pipeline(ClusterSimilarity(n_clusters=6,gamma=.1,random_state=42))\n",
    "\n",
    "sex_cluster_simil=make_pipeline(ClusterSimilarity(n_clusters=2,gamma=.1,random_state=42))\n",
    "\n",
    "preprocessing=ColumnTransformer([('num', num_pipline, num_features),\n",
    "                                ('sex',sex_cluster_simil,['sex']),\n",
    "                                ('s4',s4_cluster_simil,['s4'])])\n",
    "\n",
    "#data_prepared=preprocessing.fit_transform(Xtrain)\n",
    "#data_prepared.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for key in reg.keys():\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('reg',reg[key]),\n",
    "    ])\n",
    "    score=-cross_val_score(full_pipeline, Xtrain, ytrain, scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "    results.append((key,score.mean()/(np.max(target)-np.min(target))))\n",
    "print('models scores:',results)\n",
    "best_model_idx=np.array(results)[:,1].argmin()\n",
    "print('best model:',results[best_model_idx][0],results[best_model_idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Transformation Pipelines:**\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "out_pipeline = TransformedTargetRegressor(LinearRegression(fit_intercept=False),\n",
    "                                   transformer=StandardScaler())\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('linear_regression',out_pipeline),\n",
    "])\n",
    "full_pipeline.fit(Xtrain,ytrain)\n",
    "#LinearRegression().get_params()\n",
    "param_grid=[{'preprocessing__num__pca__n_components': [3,4,5,6,7,8],\n",
    "              'preprocessing__s4__clustersimilarity__n_clusters':[2,3,4,5,6],\n",
    "              'preprocessing__s4__clustersimilarity__gamma':[.01,.1]\n",
    "            }]\n",
    "\n",
    "grid_search=GridSearchCV(full_pipeline,param_grid,cv=10,scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(Xtrain,ytrain)\n",
    "print('best estimator=',grid_search.best_estimator_)\n",
    "print('best score=',grid_search.best_score_)\n",
    "rmse=-cross_val_score(full_pipeline,Xtrain,ytrain,scoring='neg_root_mean_squared_error',cv=10)\n",
    "rmse.mean().round(1)\n",
    "rmse_percentage=rmse.mean().round(1)/(target.max()-target.min())\n",
    "print('rmse percentage:',rmse_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "final_model=grid_search.best_estimator_\n",
    "final_prediction=final_model.predict(Xtest)\n",
    "final_rmse=mean_squared_error(y_pred=final_prediction,y_true=ytest,squared=False)\n",
    "final_rmse\n",
    "final_rmse_percentage=final_rmse.round(1)/(target.max()-target.min())\n",
    "print('final rmse percentage:',final_rmse_percentage)\n",
    "#train the best model in the whole data set including the train and test datat set.\n",
    "final_model.fit(features,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction=final_model.predict(Xtest)\n",
    "final_rmse=mean_squared_error(y_pred=final_prediction,y_true=ytest,squared=False)\n",
    "final_rmse\n",
    "final_rmse_percentage=final_rmse.round(1)/(target.max()-target.min())\n",
    "print('final rmse percentage:',final_rmse_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the final Model:\n",
    "import joblib\n",
    "\n",
    "joblib.dump(final_model,'final_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "\n",
    "\n",
    "final_model=joblib.load('final_model.pkl')\n",
    "\n",
    "new_data=features.iloc[:5]\n",
    "predictions=final_model.predict(new_data)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the end :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to use randomized search to fine tune hyper_parameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n",
    "rnd_param={\n",
    "    'preprocessing__num__pca__n_components': randint(low=2,high=6),\n",
    "    'preprocessing__s4__clustersimilarity__n_clusters':randint(low=2,high=6),\n",
    "    'preprocessing__s4__clustersimilarity__gamma':randint(low=.01,high=1)\n",
    "}\n",
    "rnd_search= RandomizedSearchCV(full_pipeline,param_distributions=rnd_param,n_iter=10,cv=3,\n",
    "                                scoring='neg_root_mean_squared_error',random_state=42)\n",
    "rnd_search.fit(Xtrain , ytrain)\n",
    "\n",
    "rnd_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try using alternative models : (XGBRegressor)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "import xgboost as xgb\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('XGB_regression',xgb.XGBRegressor(verbosity=0)),\n",
    "])\n",
    "full_pipeline.fit(Xtrain,ytrain)\n",
    "#LinearRegression().get_params()\n",
    "param_grid=[{'preprocessing__num__pca__n_components': [3,4,5,6],\n",
    "              'preprocessing__s4__clustersimilarity__n_clusters':[2,3,4,5,6,7],\n",
    "              'preprocessing__s4__clustersimilarity__gamma':[.01,.1]\n",
    "            }\n",
    "            ]\n",
    "\n",
    "grid_search=GridSearchCV(full_pipeline,param_grid,cv=10,scoring='neg_root_mean_squared_error')\n",
    "grid_search.fit(Xtrain,ytrain)\n",
    "print('best estimator=',grid_search.best_estimator_)\n",
    "print('best score=',grid_search.best_score_)\n",
    "rmse=-cross_val_score(full_pipeline,Xtrain,ytrain,scoring='neg_root_mean_squared_error',cv=10)\n",
    "rmse.mean().round(1)\n",
    "rmse_percentage=rmse.mean().round(1)/(target.max()-target.min())\n",
    "print('rmse percentage:',rmse_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try using voting regressor for the highest score estimators:\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "#**Transformation Pipelines:**\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "import xgboost as xgb\n",
    "results=[] \n",
    "reg={'LinearRegression':LinearRegression(),\n",
    "    'GradientBoostingRegressor':GradientBoostingRegressor(random_state=42),\n",
    "    #'RandomForestRegressor':RandomForestRegressor(max_features=10,n_estimators=100,random_state=42),\n",
    "    }\n",
    "    \n",
    "for key in reg.keys():\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessing),\n",
    "        ('reg',reg[key]),\n",
    "    ])\n",
    "    reg[key].fit(Xtrain,ytrain)\n",
    "\n",
    "ereg = VotingRegressor([(\"gb\", reg['GradientBoostingRegressor']), #(\"rf\", reg['RandomForestRegressor']), \n",
    "                        (\"lr\", reg['LinearRegression'])],weights=[1,2,1])\n",
    "ereg.fit(Xtrain, ytrain)\n",
    "\n",
    "mse=-cross_val_score(ereg,Xtest,ytest,scoring='neg_root_mean_squared_error',cv=10)\n",
    "rmse.mean().round(1)\n",
    "rmse_percentage=rmse.mean().round(1)/(target.max()-target.min())\n",
    "print('rmse percentage:',rmse_percentage)\n",
    "\n",
    "print('the rmse of voting regressor is worse!')\n",
    "print('best regressor is linear regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after trying altenative models we conclude that the best model to fit the data is : \n",
    "#LinearRegression with RMSE =.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    final_model, X, y, train_sizes=np.linspace(0.01, 1.0, 350), cv=10,\n",
    "    scoring=\"neg_root_mean_squared_error\")\n",
    "train_errors = train_scores.mean(axis=1)\n",
    "valid_errors = valid_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(6, 4))  # extra code – not needed, just formatting\n",
    "plt.plot(train_sizes, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "plt.plot(train_sizes, valid_errors, \"b-\", linewidth=3, label=\"valid\")\n",
    "\n",
    "# extra code – beautifies and saves Figure 4–15\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.axis([0, 400, 0, 150])\n",
    "plt.show()\n",
    "#valid_errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
